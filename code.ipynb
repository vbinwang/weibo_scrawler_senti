{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba\n",
    "import jieba.analyse\n",
    "import codecs\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import csv\n",
    "import urllib\n",
    "import random\n",
    "from pandas.core.frame import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames=['ID','Href','Blog','PubTime','Like','Comment','Transfer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('yanshuji.csv',names=colnames)\n",
    "df1['PubTime']=pd.to_datetime(df1['PubTime'])\n",
    "df1=df1[df1['PubTime']<'2018-05-25']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2= pd.read_csv('yanchunfeng.csv',names=colnames)\n",
    "df2['PubTime']=pd.to_datetime(df2['PubTime'])\n",
    "df2=df2[df2['PubTime']<'2018-05-25']\n",
    "df2=df2[df2['Blog'].str.contains('严书记')==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3= pd.read_csv('yanfuren.csv',names=colnames)\n",
    "df3['PubTime']=pd.to_datetime(df3['PubTime'])\n",
    "df3=df3[df3['PubTime']<'2018-05-25']\n",
    "df3=df3[df3['Blog'].str.contains('严书记')==False]\n",
    "df3=df3[df3['Blog'].str.contains('严春风')==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4= pd.read_csv('youeryuan.csv',names=colnames)\n",
    "df4['PubTime']=pd.to_datetime(df4['PubTime'])\n",
    "df4=df4[df4['PubTime']<'2018-05-25']\n",
    "df4=df4[df4['Blog'].str.contains('严书记')==False]\n",
    "df4=df4[df4['Blog'].str.contains('严春风')==False]\n",
    "df4=df4[df4['Blog'].str.contains('严夫人')==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all=pd.concat([df1,df2,df3,df4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all=df_all[df_all['Blog'].str.contains('优惠券')==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.to_excel('total_data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "jieba.analyse.set_stop_words('stop.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tosentence(df):\n",
    "    w1=0.2\n",
    "    w2=0.4\n",
    "    w3=0.4\n",
    "    sentence=''\n",
    "    for row in df.iterrows():\n",
    "        sentence+=row[1]['Blog']*int(w1*row[1]['Like']+w2*row[1]['Comment']+w3*row[1]['Transfer'])\n",
    "    return sentence  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 20\n"
     ]
    }
   ],
   "source": [
    "#dfbytime=[]\n",
    "file=open('senti3.txt','w')\n",
    "file.write(','.join(['Time Period','Total','Negtive','Middle','Positive','Weighted Sentiment'])+'\\n')\n",
    "for date in range(11,25):\n",
    "    for hour in range(0,24,4):\n",
    "        print(date,hour)\n",
    "        pubtime='2018-05-%d %d:00'%(date,hour%24)\n",
    "        temp1=df_all[df_all['PubTime']>pd.to_datetime(pubtime)]\n",
    "        pubtime='2018-05-%d %d:00'%(date,(hour+4)%24)\n",
    "        if not (hour+4)%24:\n",
    "            pubtime='2018-05-%d 23:59'%(date)\n",
    "        temp2=temp1[temp1['PubTime']<pd.to_datetime(pubtime)]\n",
    "        #dfbytime.append(temp2)\n",
    "        #temp2.to_excel('data_by_4hours/05-%d %d~%d.xlsx'%(date,hour,hour+4))\n",
    "        #sentence=tosentence(temp2)\n",
    "        #keywords = jieba.analyse.extract_tags(sentence, topK=20, withWeight=True)\n",
    "        #f=open('weighted_keywords/05-%d %d~%d.txt'%(date,hour,hour+4),'w')\n",
    "        #for item in keywords:\n",
    "            #f.write(item[0]+' '+str(item[1])+'\\n')\n",
    "        #f.close()\n",
    "        #sentence2=' '.join(temp2['Blog'])\n",
    "        #keywords = jieba.analyse.extract_tags(sentence2, topK=20, withWeight=True)\n",
    "        #f=open('origin_keywords/05-%d %d~%d.txt'%(date,hour,hour+4),'w')\n",
    "        #for item in keywords:\n",
    "            #f.write(item[0]+' '+str(item[1])+'\\n')\n",
    "        #f.close()\n",
    "        output=dfsummary(temp2,date,hour)\n",
    "        file.write(output)\n",
    "        file.flush()\n",
    "        file.write('\\n')\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analysis(string):\n",
    "    access_token = '24.04828717b0a4530749f1f9d33b776cf9.2592000.1569760669.282335-16947555'\n",
    "    url_origin = 'https://aip.baidubce.com/rpc/2.0/nlp/v1/sentiment_classify?charset=UTF-8&'\n",
    "    url = \"https://aip.baidubce.com/rpc/2.0/nlp/v1/sentiment_classify?access_token=\"+access_token  # API\n",
    "    headers = {'content-type': 'application/json',}\n",
    "    text = {\"text\": string}\n",
    "    analyze_response = requests.post(url = url, headers=headers, data = json.dumps(text).encode('utf-8'))\n",
    "    analyze_response.encoding = 'gbk'\n",
    "    j_analyze = json.loads(analyze_response.text)\n",
    "    return(j_analyze['items'][0]['confidence'],j_analyze['items'][0]['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dfsummary(df,date=1,hour=1):\n",
    "    w1=0.2\n",
    "    w2=0.4\n",
    "    w3=0.4\n",
    "    sumlist=[]\n",
    "    conflist=[]\n",
    "    weighted_senti=0\n",
    "    for row in df.iterrows():\n",
    "        try:\n",
    "            conf,senti= analysis(row[1]['Blog'])\n",
    "        except:\n",
    "            print('error')\n",
    "            #continue\n",
    "        sumlist.append(senti)\n",
    "        conflist.append(conf)\n",
    "        weighted_senti+=(senti-1)*(w1*row[1]['Like']+w2*row[1]['Comment']+w3*row[1]['Transfer'])*conf\n",
    "        time.sleep(random.uniform(0.5,1.0))\n",
    "    l=[sumlist,conflist]\n",
    "    l=pd.DataFrame(data=l)\n",
    "    l.to_csv('data_by_4hours/senti/05-%d %d~%d.csv'%(date,hour,hour+4))\n",
    "    return(','.join(['05-%d %d~%d'%(date,hour,hour+4),str(len(sumlist)),\n",
    "                         str(sumlist.count(0)),str(sumlist.count(1)),str(sumlist.count(2)),str(weighted_senti)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_access_token():\n",
    "    \"\"\"\n",
    "    获取百度AI平台的Access Token\n",
    "    \"\"\"\n",
    "    # 使用你的API Key及Secret Key\n",
    "    host = 'https://aip.baidubce.com/oauth/2.0/token?grant_type=client_credentials&client_id=FIGcoqTMBM95h0GlZQneRz8U&client_secret=qQs3qaPmSIlEkIPw3fIcyf0EGp2rfTXD'\n",
    "    request = urllib.request.Request(host)\n",
    "    request.add_header('Content-Type', 'application/json; charset=UTF-8')\n",
    "    response = urllib.request.urlopen(request)\n",
    "    content = response.read().decode('utf-8')\n",
    "    rdata = json.loads(content)\n",
    "    return rdata['access_token']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'24.04828717b0a4530749f1f9d33b776cf9.2592000.1569760669.282335-16947555'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_access_token()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
